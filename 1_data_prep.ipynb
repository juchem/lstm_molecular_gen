{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Prep\n",
    "\n",
    "Take a bunch of SMILES files and create numpy dataset for training (and manual testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from rdkit import Chem # conda install -c rdkit rdkit\n",
    "\n",
    "from src.features.smiles import SmilesTokenizer, cleanup_list_smiles, encode_list_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SmilesTokenizer()\n",
    "\n",
    "df0 = pd.read_csv('data/raw/hydroxychloroquine.smi', names=[\"smiles\"])\n",
    "df1 = pd.read_csv('data/raw/dataset.smi', names=[\"smiles\"])\n",
    "df2 = pd.read_csv('data/raw/hiv_inhibitors.smi', names=[\"smiles\"])\n",
    "df3 = pd.read_csv('data/raw/known_TRPM8-inhibitors.smi', names=[\"smiles\"])\n",
    "df4 = pd.read_csv('data/raw/manual_testing.smi', names=[\"smiles\"])\n",
    "\n",
    "df_train = pd.concat([df0,\n",
    "                      df1,\n",
    "                      df2,\n",
    "                      df3])\n",
    "\n",
    "print('Training set (original):', len(df_train))\n",
    "\n",
    "# limit training set to 200 character sequences\n",
    "df_train = df_train.loc[df_train['smiles'].str.len() <= 200]\n",
    "\n",
    "print('Training set (trimmed at 200 chars):', len(df_train))\n",
    "\n",
    "df_test = df4\n",
    "\n",
    "print('Test set:', len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, encode, pad, export training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(data):\n",
    "    X = []\n",
    "    for t in data:\n",
    "        #print(t)\n",
    "        for i in range(len(t) - 1):\n",
    "            m = t[0:i+2]\n",
    "            #print(m)\n",
    "            X.append(m)\n",
    "    return X\n",
    "\n",
    "smiles = cleanup_list_smiles(list(df_train['smiles']))\n",
    "encoded_smiles = encode_list_smiles(smiles)\n",
    "#encoded_smiles = window_data(encoded_smiles)\n",
    "\n",
    "encoded_smiles = window_data(encoded_smiles)\n",
    "\n",
    "dataset_train = pad_sequences(encoded_smiles, maxlen=None, dtype='float32', padding='pre', value=st.zero())\n",
    "\n",
    "print(dataset_train.shape)\n",
    "\n",
    "np.save('data/interim/smiles_train.npy', dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean, encode, pad, export manual testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = cleanup_list_smiles(list(df_test['smiles']))\n",
    "encoded_smiles = encode_list_smiles(smiles)\n",
    "encoded_smiles = window_data(encoded_smiles)\n",
    "\n",
    "dataset_test = pad_sequences(encoded_smiles, maxlen=None, dtype='float32', padding='pre', value=st.zero())\n",
    "\n",
    "print(dataset_test.shape)\n",
    "\n",
    "np.save('data/interim/smiles_test.npy', dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
